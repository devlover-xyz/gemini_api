# Scraping API

A reusable web scraping API built with Bun and Puppeteer.

## Features

- ğŸ”¥ Built with Bun for fast performance
- ğŸ¯ Reusable scraper architecture with base class
- ğŸ”Œ Easy to add new scrapers
- ğŸŒ RESTful API with native Bun HTTP server
- ğŸ“¦ TypeScript support
- ğŸ¨ Clean architecture with separation of concerns
- ğŸ¤– reCAPTCHA solver support (manual, 2captcha, anti-captcha, extension)
- ğŸ¥· Stealth mode using puppeteer-extra-plugin-stealth for anti-detection
- ğŸ­ Random User-Agent rotation for each request
- ğŸŠ Browser pooling for concurrent requests
- âš¡ Request queue and rate limiting
- ğŸ”„ Auto-retry with exponential backoff
- ğŸ³ Docker-ready with production optimizations
- ğŸ“Š Built-in monitoring and metrics
- âœ… Implements Puppeteer best practices

## Project Structure

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ BaseScraper.ts       # Abstract base class for scrapers
â”‚   â”‚   â””â”€â”€ ScraperManager.ts    # Manages and executes scrapers
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ ExampleScraper.ts    # Example scraper implementation
â”‚   â”‚   â”œâ”€â”€ GoogleSearchScraper.ts # Google search scraper
â”‚   â”‚   â””â”€â”€ index.ts             # Register scrapers here
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ scraper.routes.ts    # API route handlers
â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â””â”€â”€ scraper.ts           # TypeScript interfaces
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ response.ts          # Response helpers
â”‚   â””â”€â”€ index.ts                 # Main server file
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

## Installation

```bash
bun install
```

## Environment Variables

Create a `.env` file:

```env
PORT=3000
NODE_ENV=development
HEADLESS=true
```

## Usage

### Start the server

Development mode with hot reload:
```bash
bun run dev
```

Production mode:
```bash
bun run start
```

### API Endpoints

#### Get all registered scrapers
```bash
GET /api/scrapers
```

Response:
```json
{
  "scrapers": ["example", "google-search"]
}
```

#### Execute scraper (POST)
```bash
POST /api/scrape/:scraperName
Content-Type: application/json

{
  "params": {
    "url": "https://example.com"
  },
  "config": {
    "headless": true,
    "timeout": 30000
  }
}
```

#### Execute scraper (GET with query params)
```bash
GET /api/scrape/example?url=https://example.com
```

Response:
```json
{
  "success": true,
  "data": {
    "title": "Example Domain",
    "description": "Example description",
    "url": "https://example.com"
  },
  "timestamp": "2025-10-06T10:30:00.000Z",
  "duration": 2341
}
```

## Creating a New Scraper

1. Create a new file in `src/scrapers/` directory:

```typescript
// src/scrapers/MyScraper.ts
import { BaseScraper } from '../core/BaseScraper';
import type { ScraperParams } from '../types/scraper';

interface MyData {
  // Define your data structure
  title: string;
  items: string[];
}

export class MyScraper extends BaseScraper<MyData> {
  protected async scrape(params: ScraperParams): Promise<MyData> {
    const { url } = params;

    // Navigate to URL
    await this.navigateToUrl(url);

    // Wait for elements
    await this.waitForSelector('.content');

    // Extract data
    const title = await this.getTextContent('h1');
    const items = await this.getAllTextContent('.item');

    return {
      title: title || '',
      items,
    };
  }
}
```

2. Register your scraper in `src/scrapers/index.ts`:

```typescript
import { MyScraper } from './MyScraper';

export function registerScrapers() {
  scraperManager.register('example', ExampleScraper);
  scraperManager.register('google-search', GoogleSearchScraper);
  scraperManager.register('my-scraper', MyScraper); // Add this
}
```

3. Use it via API:

```bash
POST /api/scrape/my-scraper
{
  "params": {
    "url": "https://example.com"
  }
}
```

## Available Helper Methods in BaseScraper

- `navigateToUrl(url, waitUntil)` - Navigate to a URL
- `waitForSelector(selector, timeout)` - Wait for element to appear
- `getTextContent(selector)` - Get text content of an element
- `getAllTextContent(selector)` - Get text content of all matching elements
- `getAttribute(selector, attribute)` - Get attribute value
- `takeScreenshot(path)` - Take screenshot
- `hasRecaptcha()` - Check if reCAPTCHA is present on page
- `solveRecaptcha()` - Solve reCAPTCHA automatically
- `page` - Access Puppeteer page instance directly

## Anti-Detection & reCAPTCHA Support

âœ… **Stealth Mode Enabled** - Uses `puppeteer-extra-plugin-stealth` to bypass bot detection
âœ… **reCAPTCHA Detection: 100%** - Proper Puppeteer best practices

This API includes:
- **Stealth Mode**: Automatically bypasses most bot detection systems. See [STEALTH.md](STEALTH.md)
- **reCAPTCHA Solver**: Automatic reCAPTCHA solving with multiple providers. See [RECAPTCHA.md](RECAPTCHA.md)

### Quick Start

```bash
# Test detection only (no solver needed)
curl -X POST http://localhost:3000/api/scrape/google-recaptcha-demo

# Test with manual solving
curl -X POST http://localhost:3000/api/scrape/google-recaptcha-demo \
  -H "Content-Type: application/json" \
  -d '{
    "config": {
      "headless": false,
      "recaptcha": {"enabled": true, "provider": "manual"}
    }
  }'

# Test with 2Captcha
curl -X POST http://localhost:3000/api/scrape/recaptcha-test \
  -H "Content-Type: application/json" \
  -d '{
    "params": {"url": "https://example.com"},
    "config": {
      "recaptcha": {
        "enabled": true,
        "provider": "2captcha",
        "apiKey": "your_api_key"
      }
    }
  }'
```

### Supported Methods

- **Extension** - Free (experimental), browser extension-based
- **Manual** - Free, requires human intervention
- **2Captcha** - Paid (~$2.99/1000), fully automated
- **Anti-Captcha** - Paid (~$2/1000), fully automated

### Detection Features

- âœ… Multiple wait strategies (`waitForSelector`, `waitForFunction`)
- âœ… Resource whitelisting (reCAPTCHA, gstatic.com)
- âœ… Anti-detection measures
- âœ… Fallback detection methods
- âœ… Support for v2, v3, hCaptcha

See [SOLUTION.md](SOLUTION.md) for technical details.

## Example Requests

### Example Scraper
```bash
curl -X POST http://localhost:3000/api/scrape/example \
  -H "Content-Type: application/json" \
  -d '{"params": {"url": "https://example.com"}}'
```

### Google Search Scraper
```bash
curl -X POST http://localhost:3000/api/scrape/google-search \
  -H "Content-Type: application/json" \
  -d '{"params": {"query": "bun javascript", "limit": 5}}'
```

Or with GET:
```bash
curl "http://localhost:3000/api/scrape/google-search?query=bun+javascript&limit=5"
```

## Configuration

Each scraper can be configured with:

- `headless` - Run browser in headless mode (default: true)
- `timeout` - Navigation timeout in ms (default: 30000)
- `userAgent` - Custom user agent (default: random realistic UA)
- `viewport` - Browser viewport size (default: 1920x1080)

Example:
```json
{
  "params": {"url": "https://example.com"},
  "config": {
    "headless": false,
    "timeout": 60000,
    "userAgent": "Custom User Agent",
    "viewport": {"width": 1280, "height": 720}
  }
}
```

**Note**: If `userAgent` is not specified, each request will use a different random realistic user agent from the `user-agents` package. This helps avoid detection by preventing user agent-based tracking.

## Troubleshooting

Having issues? Check the [TROUBLESHOOTING.md](TROUBLESHOOTING.md) guide for common problems and solutions:

- Google blocking with reCAPTCHA
- Page errors (`solveSimpleChallenge is not defined`)
- Request timeouts
- Navigation errors
- And more...

## License

MIT
